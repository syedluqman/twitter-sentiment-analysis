{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>This notebook does the following. </p>\n",
    "<p>Part 1: Scrapes the twitter and download the tweets mentioning the 'ChatGPT'. </p>\n",
    "<p>Part 2: Build a sentiment analysis model and identify tweets as positive and negative. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This code requires twitter API, and access token. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# add your Twitter API credentials here\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# authenticate with the Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# create the API object\n",
    "api = tweepy.API(auth)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save all tweets that mention ChatGPT in a .json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for tweets that mention ChatGPT\n",
    "tweets = api.search(q=\"ChatGPT\", lang=\"en\", count=100)\n",
    "\n",
    "# extract the text of each tweet\n",
    "tweet_texts = [tweet.text for tweet in tweets]\n",
    "\n",
    "# save the tweets to a JSON file\n",
    "with open('tweets.json', 'w') as f:\n",
    "    json.dump(tweet_texts, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> PArt Two - Sentiment Analysis</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "\n",
    "# load the tweets from the JSON file\n",
    "with open('tweets.json', 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "\n",
    "# set the maximum number of words to use in the tokenizer\n",
    "max_words = 10000\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "\n",
    "# fit the tokenizer on the tweets\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# convert the text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "# set the maximum length of the sequences\n",
    "max_length = 100\n",
    "\n",
    "# pad the sequences to a fixed length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> You can use your own data or a tensorflow pretrained data to do the classification. I am using TensorFlow Hub's data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained model from TensorFlow Hub\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(max_length,), dtype=tf.int32),\n",
    "    keras.layers.Embedding(max_words, 16),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.load_weights('https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Finally, predict the sentiment of each tweet. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the sentiment of each tweet\n",
    "predictions = model.predict(padded_sequences)\n",
    "\n",
    "# classify the tweets as positive or negative\n",
    "sentiments = ['positive' if prediction >= 0.5 else 'negative' for prediction in predictions]\n",
    "\n",
    "# print the results\n",
    "for i in range(len(tweets)):\n",
    "    print(f'{sentiments[i]}: {tweets[i]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
